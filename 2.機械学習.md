# 機械学習
## 要点のまとめ
#### 0.はじめに
* 学習の目的  
  ・基本的なモデリング手法の理解  
  ・機械学習モデリングの流れの理解  
  
* 機械学習とは  
  タスクTを性能指標Pで測定し、その性能が過去の経験Eにより改善される時  
  T, Pに関してEから学習する、と言う。  
  (Tはアプリにさせたいこと、Eはデータのイメージ)

* 機械学習モデリングプロセス  
  （１）問題設定  
  （２）データ選定  
  （３）データの前処理  
  （４）機械学習モデルの選択  
  （５）モデルの学習  
  （６）モデルの評価  
  <br>
  \*そもそも（１）の時点で機械学習でなくて良い、となる場合もある。  
  <br>

* 機械学習のデメリット  
  ・技術負債が大きい  
  ・テストが難しいケースがある  
<br>
  
#### 1.線形回帰モデル
* 線形回帰モデルとは  
  ・回帰問題用のモデル  
  ・回帰問題：ある入力（離散値or連続値）から出力(連続値)を予測する問題のこと  
  ・回帰で扱うデータ  
    （ⅰ）入力：説明変数、あるいは特徴量と呼ぶ(m次元ベクトル)  
    （ⅱ）出力：目的変数(スカラー値)  
    <br>
    　以下のように表すことがある。  
    　説明変数：<img src="https://latex.codecogs.com/gif.latex?x=\left&space;(&space;x_{1},&space;x_{2},...,x_{m}&space;\right&space;)^{T}\in&space;\mathbb{R}^{m}" title="x=\left ( x_{1}, x_{2},...,x_{m} \right )^{T}\in \mathbb{R}^{m}" />  
    　目的変数：<img src="https://latex.codecogs.com/gif.latex?y\in&space;\mathbb{R}^{1}" title="y\in \mathbb{R}^{1}" />  

  ・教師あり学習  
   　教師データ：<img src="https://latex.codecogs.com/gif.latex?\left&space;\{&space;\left&space;(&space;x_{i},y_{i}&space;\right&space;);i=1,2,...,n&space;\right&space;\}" title="\left \{ \left ( x_{i},y_{i} \right );i=1,2,...,n \right \}" />  
 
  ・入力とm次元パラメータの線形結合を出力するモデル
  　慣例として、教師データと区別するために予測値にはハットをつける。(ex. <img src="https://latex.codecogs.com/gif.latex?\hat{y}" title="\hat{y}" />)  
  <br>
  　パラメータ：各特徴量が予測値に対してどのように影響を与えるかを決定する「重み」の集合  
   　<img src="https://latex.codecogs.com/gif.latex?w=\left&space;(&space;w_{1},w_{2},...,w_{m}&space;\right&space;)^{T}\in&space;\mathbb{R}^{m}" title="w=\left ( w_{1},w_{2},...,w_{m} \right )^{T}\in \mathbb{R}^{m}" />  
    <br>
  　線形結合：入力とパラメータの積+切片  
   　<img src="https://latex.codecogs.com/gif.latex?\hat{y}={\mathbf{w}}^{T}\mathbf{x}&plus;w_{0}=\sum_{j=1}^{m}w_{j}x_{j}&plus;w_{0}" title="\hat{y}={\mathbf{w}}^{T}\mathbf{x}+w_{0}=\sum_{j=1}^{m}w_{j}x_{j}+w_{0}" />  
   
  ・説明変数が１次元の場合、単回帰モデルと呼ぶ。  
  ・説明変数が多次元(2次元以上)の場合、重回帰モデルと呼ぶ。  
  
* データの分割とモデルの汎化性能  
  ・データの分割
  　学習用データ：学習に用いるデータ  
  　検証用データ：学習済みモデルの精度を検証するためのデータ  
   <br>
  ・分割する理由  
  　モデルの汎化性能を図るため。  
  　汎化性能を測るには未知のデータが必要。そのため、データの一部を検証用に残しておく。  
  
* 最小二乗法  
  ・パラメータの推定は最小二乗法を用いる。  
  ・平均二乗誤差(MSE)を最小にするパラメータを提案することで学習を進める。  
  
* 最尤法  
  ・線型回帰は最尤法でも解析的にパラメータの推定が可能。  
  　その際の結果は、最小二乗法を用いた結果と一致する。  
<br>

#### 2.非線形回帰モデル
* 非線形回帰モデルとは  
  複雑な非線形構造を内在する現象に対して使用するモデル  
  
* 基底展開法  
  ・回帰関数として基底関数を使用する手法  
  ・既知の非線形関数とパラメータベクトルの線型結合を行う。  
  ・未知パラメータは最小二乗法や最尤法で推定(線型回帰と同様)  
  <br>
  <img src="https://latex.codecogs.com/gif.latex?y_{i}=f\left&space;(&space;\mathbf{x}_{i}&space;\right&space;)&plus;\epsilon&space;_{i}=w_{0}&plus;\sum_{j=1}^{m}w_{j}\phi&space;_{j}(\textbf{x}_{i})&plus;\epsilon&space;_{i}" title="y_{i}=f\left ( \mathbf{x}_{i} \right )+\epsilon _{i}=w_{0}+\sum_{j=1}^{m}w_{j}\phi _{j}(\textbf{x}_{i})+\epsilon _{i}" />  
  <br>
  \*代表的な基底関数：多項式関数、ガウス型基底関数、スプライン関数  
  
* 未学習と過学習の対策  
  ・未学習の対策  
  　表現力がより高いモデルを使用する。  
  ・過学習の対策  
  　学習データの数を増やす  
  　不要な基底関数(変数)を削除して表現力を抑止する。  
  　正則化法を利用して表現力を抑止する。  
   
* 正則化法 (罰則化法)  
  ・モデルの複雑さに伴って、その値が大きくなる「正則化項」を課した関数を用意  
  　その値を最小化することで過学習を抑制する手法。  
  ・正則化項(罰則化項)  
  　形状によって種類があり、それぞれ推定量の性質が異なる。  
  　<br>
  　<img src="https://latex.codecogs.com/gif.latex?S_{r}=\left&space;(&space;\mathbf{y}-\Phi&space;\mathbf{w}\right&space;)^{T}\left&space;(&space;\mathbf{y}-\Phi&space;\mathbf{w}\right&space;)&plus;\gamma&space;R(\mathbf{w})" title="S_{r}=\left ( \mathbf{y}-\Phi \mathbf{w}\right )^{T}\left ( \mathbf{y}-\Phi \mathbf{w}\right )+\gamma R(\mathbf{w})" />  
   <br>
   　　<img src="https://latex.codecogs.com/gif.latex?\gamma&space;R(\mathbf{w})" title="\gamma R(\mathbf{w})" /> ： 正則化項  
   　　<img src="https://latex.codecogs.com/gif.latex?\gamma" title="\gamma" /> ： 正則パラメータ  
  
  ・正則化項の役割  
  　なし： 最小二乗法  
  　L2ノルムを利用： Ridge推定量  
  　L1ノルムを利用： Lasso推定量  
   
  ・正則化パラメータの役割  
  　大きくする： 制約が強くなる(罰則がキツくなるイメージ)  
  　小さくする： 制約が緩くなる(0なら正則化項が消失→罰則なし)  
   
* モデルの選択  
  ・基底関数の個数  
  ・基底関数の位置  
  ・基底関数のバンド幅  
  ・正則化パラメータ  
  　これらはどうやって決める？ →　交差検証法(クロスバリデーション)
  
* ホールドアウト法  
  ・手元にあるデータを、学習用と検証用の2つに分ける手法。  
  ・問題点  
  　大量のデータがないと有効でない。(学習用の割合を増やしすぎるとテストの精度が落ちる。逆も然り。)  
  　分けたデータに偏りがあると学習がうまくいかない。(エラーデータが検証データに固まると、精度が悪く出てしまうなど)  
   
* 交差検証法(クロスバリデーション)  
  ・データをk個グループに分けて1つを検証用、残りを学習用とする。  
  ・イテレータごとに検証用のデータを変えて、k回だけイテレータを回す。(全データが１度は検証用として使われる。)  
  ・各イテレータで出た精度の平均をCV値と言い、そのモデルの汎化性能を表す。  
  
  　\*計算量は増えてしまうが、少ないデータで精度の高いモデルの評価が行える。  
<br>

#### 3.ロジスティック回帰モデル
* ロジスティック回帰モデルとは  
  ・分類問題に対するモデル  
  ・分類問題で扱うデータ  
    （ⅰ）入力： m次元ベクトル  
    　<img src="https://latex.codecogs.com/gif.latex?x=\left&space;(&space;x_{1},&space;x_{2},...,x_{m}&space;\right&space;)^{T}\in&space;\mathbb{R}^{m}" title="x=\left ( x_{1}, x_{2},...,x_{m} \right )^{T}\in \mathbb{R}^{m}" />  
    （ⅱ）出力： 0 or 1 の値  
    　<img src="https://latex.codecogs.com/gif.latex?y\in&space;\left&space;\{&space;0,&space;1&space;\right&space;\}" title="y\in \left \{ 0, 1 \right \}" />  
    (ⅲ)教師データ: 0 or 1 の値  
    　<img src="https://latex.codecogs.com/gif.latex?\left&space;\{&space;\left&space;(&space;\mathbf{x}_{i},&space;y_{i}&space;\right&space;);&space;i&space;=1,2,...,n&space;\right&space;\}" title="\left \{ \left ( \mathbf{x}_{i}, y_{i} \right ); i =1,2,...,n \right \}" />  

  ・入力とm次元パラメータの線形結合をシグモイド関数に入力する。  
  　出力はy=1となる確率。  
  ・パラメータが変わるとシグモイド関数の形が変わるイメージ。  
  ・シグモイド関数の性質(連鎖律)  
  　シグモイド関数の微分はシグモイド関数自身で表せる。  
  　数ある0〜1出力の関数からこの関数が選ばれるのはこれが理由(最尤法の微分時に有用)  
    
  ・出力について  
  　<img src="https://latex.codecogs.com/gif.latex?P\left&space;(&space;\mathbf{Y}=1\mid&space;\mathbf{x}&space;\right&space;)=\sigma&space;(w_{0}&plus;w_{1}x_{1}&plus;...&plus;w_{m}x_{m})" title="P\left ( \mathbf{Y}=1\mid \mathbf{x} \right )=\sigma (w_{0}+w_{1}x_{1}+...+w_{m}x_{m})" />  
   　Yは確率Pが0.5以上なら1と予測。0.5未満なら0と予測する。  
    
* 最尤推定  
  ・データを元にその分布のパラメータを推定するという手法  
  ・尤度関数  
  　データを固定し、パラメータを変化させる。
  　尤度関数を最大化するようなパラメータを選ぶ推定方法を最尤推定という。
   <br>
   　<a href="https://www.codecogs.com/eqnedit.php?latex=P\left&space;(&space;y_{1},y_{2},...,y_{n};p&space;\right&space;)=\prod_{i=1}^{n}p^{y_i}(1-p)^{i-y_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P\left&space;(&space;y_{1},y_{2},...,y_{n};p&space;\right&space;)=\prod_{i=1}^{n}p^{y_i}(1-p)^{i-y_i}" title="P\left ( y_{1},y_{2},...,y_{n};p \right )=\prod_{i=1}^{n}p^{y_i}(1-p)^{i-y_i}" /></a>  
    　pを未知数として、左辺で得られたデータを元に尤もらしいpを求める。  
   <br>
  ・尤度関数を最大とするパラメータの探索  
  　微分が楽になるため、対数を取る場合が多い。(最大点は変わらないため問題なし)
   
 * 確率的勾配降下法(SGD)  
  ・データを1つランダムに選んでパラメータを更新する手法。  
  ・通常の勾配降下法より1回のパラメータ更新に必要な計算量を抑えられる。  
  
 * モデルの性能指標  
  ・混同行列  
  　モデルの予測結果を4つの観点で分類したもの。  
  　各結果に当てはまる個数を記載する。  
   ![](image/機械学習/1.png)  
   


s
s


ss
s
