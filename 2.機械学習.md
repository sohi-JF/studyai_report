# 機械学習
## 要点のまとめ
#### 0.はじめに
* 学習の目的  
  ・基本的なモデリング手法の理解  
  ・機械学習モデリングの流れの理解  
  
* 機械学習とは  
  タスクTを性能指標Pで測定し、その性能が過去の経験Eにより改善される時  
  T, Pに関してEから学習する、と言う。  
  (Tはアプリにさせたいこと、Eはデータのイメージ)

* 機械学習モデリングプロセス  
  （１）問題設定  
  （２）データ選定  
  （３）データの前処理  
  （４）機械学習モデルの選択  
  （５）モデルの学習  
  （６）モデルの評価  
  <br>
  \*そもそも（１）の時点で機械学習でなくて良い、となる場合もある。  
  <br>

* 機械学習のデメリット  
  ・技術負債が大きい  
  ・テストが難しいケースがある  
<br>
  
#### 1.線形回帰モデル
* 線形回帰モデルとは  
  ・回帰問題用のモデル  
  ・回帰問題：ある入力（離散値or連続値）から出力(連続値)を予測する問題のこと  
  ・回帰で扱うデータ  
    （ⅰ）入力：説明変数、あるいは特徴量と呼ぶ(m次元ベクトル)  
    （ⅱ）出力：目的変数(スカラー値)  
    <br>
    　以下のように表すことがある。  
    　説明変数：<img src="https://latex.codecogs.com/gif.latex?x=\left&space;(&space;x_{1},&space;x_{2},...,x_{m}&space;\right&space;)^{T}\in&space;\mathbb{R}^{m}" title="x=\left ( x_{1}, x_{2},...,x_{m} \right )^{T}\in \mathbb{R}^{m}" />  
    　目的変数：<img src="https://latex.codecogs.com/gif.latex?y\in&space;\mathbb{R}^{1}" title="y\in \mathbb{R}^{1}" />  

  ・教師あり学習  
   　教師データ：<img src="https://latex.codecogs.com/gif.latex?\left&space;\{&space;\left&space;(&space;x_{i},y_{i}&space;\right&space;);i=1,2,...,n&space;\right&space;\}" title="\left \{ \left ( x_{i},y_{i} \right );i=1,2,...,n \right \}" />  
 
  ・入力とm次元パラメータの線形結合を出力するモデル
  　慣例として、教師データと区別するために予測値にはハットをつける。(ex. <img src="https://latex.codecogs.com/gif.latex?\hat{y}" title="\hat{y}" />)  
  <br>
  　パラメータ：各特徴量が予測値に対してどのように影響を与えるかを決定する「重み」の集合  
   　<img src="https://latex.codecogs.com/gif.latex?w=\left&space;(&space;w_{1},w_{2},...,w_{m}&space;\right&space;)^{T}\in&space;\mathbb{R}^{m}" title="w=\left ( w_{1},w_{2},...,w_{m} \right )^{T}\in \mathbb{R}^{m}" />  
    <br>
  　線形結合：入力とパラメータの積+切片  
   　<img src="https://latex.codecogs.com/gif.latex?\hat{y}={\mathbf{w}}^{T}\mathbf{x}&plus;w_{0}=\sum_{j=1}^{m}w_{j}x_{j}&plus;w_{0}" title="\hat{y}={\mathbf{w}}^{T}\mathbf{x}+w_{0}=\sum_{j=1}^{m}w_{j}x_{j}+w_{0}" />  
   
  ・説明変数が１次元の場合、単回帰モデルと呼ぶ。  
  ・説明変数が多次元(2次元以上)の場合、重回帰モデルと呼ぶ。  
  
* データの分割とモデルの汎化性能  
  ・データの分割
  　学習用データ：学習に用いるデータ  
  　検証用データ：学習済みモデルの精度を検証するためのデータ  
   <br>
  ・分割する理由  
  　モデルの汎化性能を図るため。  
  　汎化性能を測るには未知のデータが必要。そのため、データの一部を検証用に残しておく。  
  
* 最小二乗法  
  ・パラメータの推定は最小二乗法を用いる。  
  ・平均二乗誤差(MSE)を最小にするパラメータを提案することで学習を進める。  
  
* 最尤法  
  ・線型回帰は最尤法でも解析的にパラメータの推定が可能。  
  　その際の結果は、最小二乗法を用いた結果と一致する。  
<br>

#### 2.非線形回帰モデル
* 非線形回帰モデルとは  
  複雑な非線形構造を内在する現象に対して使用するモデル  
  
* 基底展開法  
  ・回帰関数として基底関数を使用する手法  
  ・既知の非線形関数とパラメータベクトルの線型結合を行う。  
  ・未知パラメータは最小二乗法や最尤法で推定(線型回帰と同様)  
  <br>
  <img src="https://latex.codecogs.com/gif.latex?y_{i}=f\left&space;(&space;\mathbf{x}_{i}&space;\right&space;)&plus;\epsilon&space;_{i}=w_{0}&plus;\sum_{j=1}^{m}w_{j}\phi&space;_{j}(\textbf{x}_{i})&plus;\epsilon&space;_{i}" title="y_{i}=f\left ( \mathbf{x}_{i} \right )+\epsilon _{i}=w_{0}+\sum_{j=1}^{m}w_{j}\phi _{j}(\textbf{x}_{i})+\epsilon _{i}" />  
  <br>
  \*代表的な基底関数：多項式関数、ガウス型基底関数、スプライン関数  
  
* 未学習と過学習の対策  
  ・未学習の対策  
  　表現力がより高いモデルを使用する。  
  ・過学習の対策  
  　学習データの数を増やす  
  　不要な基底関数(変数)を削除して表現力を抑止する。  
  　正則化法を利用して表現力を抑止する。  
   
 * 正則化法 (罰則化法)  
  ・モデルの複雑さに伴って、その値が大きくなる「正則化項」を課した関数を用意  
  　その値を最小化することで過学習を抑制する手法。  
  ・正則化項(罰則化項)  
  　形状によって種類があり、それぞれ推定量の性質が異なる。  
  　<br>
  　<img src="https://latex.codecogs.com/gif.latex?S_{r}=\left&space;(&space;\mathbf{y}-\Phi&space;\mathbf{w}\right&space;)^{T}\left&space;(&space;\mathbf{y}-\Phi&space;\mathbf{w}\right&space;)&plus;\gamma&space;R(\mathbf{w})" title="S_{r}=\left ( \mathbf{y}-\Phi \mathbf{w}\right )^{T}\left ( \mathbf{y}-\Phi \mathbf{w}\right )+\gamma R(\mathbf{w})" />  
   <br>
   　　<img src="https://latex.codecogs.com/gif.latex?\gamma&space;R(\mathbf{w})" title="\gamma R(\mathbf{w})" /> ： 正則化項  
   　　<img src="https://latex.codecogs.com/gif.latex?\gamma" title="\gamma" /> ： 正則パラメータ  
  
  ・正則化項の役割  
  　なし： 最小二乗法  
  　L2ノルムを利用： Ridge推定量  
  　L1ノルムを利用： Lasso推定量  
   
  ・正則化パラメータの役割  
  　大きくする： 制約が強くなる(罰則がキツくなるイメージ)  
  　小さくする： 制約が緩くなる(0なら正則化項が消失→罰則なし)  
   
 * モデルの選択  
  
  
  
s
s


ss
s
