# 深層学習 Day1, 2
## 要点のまとめ

#### Day1-0.ニューラルネットワークの全体像

* 識別モデルと生成モデル  
  ・識別モデル  
  　目的：データを目的のクラスに分類する。(データ→クラス)  
  　計算結果：<img src="https://latex.codecogs.com/gif.latex?p(C_{k}\mid&space;x)" title="p(C_{k}\mid x)" />  

  ・生成モデル  
  　目的：特定のクラスのデータを生成する。(クラス→データ)  
  　計算結果：<img src="https://latex.codecogs.com/gif.latex?p(x\mid&space;C_{k})" title="p(x\mid C_{k})" />  

* 識別器の開発アプローチ  
  ・生成モデル  
  　<img src="https://latex.codecogs.com/gif.latex?p(x\mid&space;C_{k})\cdot&space;p(C_{k})" title="p(x\mid C_{k})\cdot p(C_{k})" />を推定する。  
  ・識別モデル  
  　<img src="https://latex.codecogs.com/gif.latex?p(C_{k}\mid&space;x)" title="p(C_{k}\mid x)" />を推定する。  
  ・識別関数  
  　入力値xを直接クラスに写像する関数f(x)を推定する。  
  　データをクラスに分類する点では識別モデルと同じだが、クラスに属する確率はわからない。(最も確率の高いクラスのが出力)  
 
 * ニューラルネットワーク(NN)でできること  
  ・回帰：結果予測、ランキングなど  
  ・分類：写真の判別、文字の認識、種類の分類など  
  ・一般に中間層が4層以上のNNを深層NNと呼ぶ。  

#### Day1-1.入力層〜中間層

* 入力層と中間層  
  ・入力層に入ってくる入力値は、重みによる補正を受け中間層に伝わる。  
  ・各入力層から出力された値は総和され、さらにバイアスが加わった上で中間層の入力値となる。  
  ・中間層に入力された値は、活性化関数を通され中間層の出力となる。  
  ・入力層の出力  
  　<img src="https://latex.codecogs.com/gif.latex?w_{i}x_{i}" title="w_{i}x_{i}" />  
  ・中間層の出力  
  　<img src="https://latex.codecogs.com/gif.latex?z=f(u)" title="z=f(u)" />  
  　<img src="https://latex.codecogs.com/gif.latex?u=\sum_{i=1}^{n}w_{i}x_{i}&plus;b" title="u=\sum_{i=1}^{n}w_{i}x_{i}+b" />  

#### Day1-2.活性化関数

* 活性化関数とは  
  ・NNにおいて、次の層への出力の大きさを決める非線形の関数のこと。  
  ・入力値に応じて、次層への伝達のON/OFFあるいは強弱を定義できる。  
  
* 中間層用の活性化関数  
  ・ステップ関数  
  　閾値を超えたら発火する関数。  
  　出力は常に0か1になる。  
  　過去にパーセプトロンで使われた実績があるが、現在はほとんど使われることはない。  
  　<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;1\,\,\,\,(x\geq&space;0)\\0\,\,\,\,(x<&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} 1\,\,\,\,(x\geq 0)\\0\,\,\,\,(x< 0) \end{matrix}\right." />  
   
  ・シグモイド関数  
  　0〜1の間を緩やかに変化する関数。  
  　信号の強弱を伝えられ、このことがNN普及のきっかけとなった。  
  　ただし、勾配消失問題を引き起こす課題がある。  
  　<img src="https://latex.codecogs.com/gif.latex?f(u)=\frac{1}{1&plus;exp(-u)}" title="f(u)=\frac{1}{1+exp(-u)}" />  
   
  ・ReLU関数  
  　今最も使われている活性化関数  
  　シグモイド関数が抱えていた勾配消失問題の回避と、スパース化に貢献することで良い成果をもたらしている。  
  　<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;x\,\,\,\,(x>&space;0)\\&space;0\,\,\,\,(x\leq&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} x\,\,\,\,(x> 0)\\ 0\,\,\,\,(x\leq 0) \end{matrix}\right." />  
   
#### Day1-3.出力層

* 
   
   
   
s
s
s
