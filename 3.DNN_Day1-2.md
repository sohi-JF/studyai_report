# 深層学習 Day1, 2
## 要点のまとめ

#### Day1-0.ニューラルネットワークの全体像

* 識別モデルと生成モデル  
  ・識別モデル  
  　目的：データを目的のクラスに分類する。(データ→クラス)  
  　計算結果：<img src="https://latex.codecogs.com/gif.latex?p(C_{k}\mid&space;x)" title="p(C_{k}\mid x)" />  

  ・生成モデル  
  　目的：特定のクラスのデータを生成する。(クラス→データ)  
  　計算結果：<img src="https://latex.codecogs.com/gif.latex?p(x\mid&space;C_{k})" title="p(x\mid C_{k})" />  

* 識別器の開発アプローチ  
  ・生成モデル  
  　<img src="https://latex.codecogs.com/gif.latex?p(x\mid&space;C_{k})\cdot&space;p(C_{k})" title="p(x\mid C_{k})\cdot p(C_{k})" />を推定する。  
  ・識別モデル  
  　<img src="https://latex.codecogs.com/gif.latex?p(C_{k}\mid&space;x)" title="p(C_{k}\mid x)" />を推定する。  
  ・識別関数  
  　入力値xを直接クラスに写像する関数f(x)を推定する。  
  　データをクラスに分類する点では識別モデルと同じだが、クラスに属する確率はわからない。(最も確率の高いクラスのが出力)  
 
 * ニューラルネットワーク(NN)でできること  
  ・回帰：結果予測、ランキングなど  
  ・分類：写真の判別、文字の認識、種類の分類など  
  ・一般に中間層が4層以上のNNを深層NNと呼ぶ。  

#### Day1-1.入力層〜中間層

* 入力層と中間層  
  ・入力層に入ってくる入力値は、重みによる補正を受け中間層に伝わる。  
  ・各入力層から出力された値は総和され、さらにバイアスが加わった上で中間層の入力値となる。  
  ・中間層に入力された値は、活性化関数を通され中間層の出力となる。  
  ・入力層の出力  
  　<img src="https://latex.codecogs.com/gif.latex?w_{i}x_{i}" title="w_{i}x_{i}" />  
  ・中間層の出力  
  　<img src="https://latex.codecogs.com/gif.latex?z=f(u)" title="z=f(u)" />  
  　<img src="https://latex.codecogs.com/gif.latex?u=\sum_{i=1}^{n}w_{i}x_{i}&plus;b" title="u=\sum_{i=1}^{n}w_{i}x_{i}+b" />  

#### Day1-2.活性化関数

* 活性化関数とは  
  ・NNにおいて、次の層への出力の大きさを決める非線形の関数のこと。  
  ・入力値に応じて、次層への伝達のON/OFFあるいは強弱を定義できる。  
  
* 中間層用の活性化関数  
  ・ステップ関数  
  　閾値を超えたら発火する関数。  
  　出力は常に0か1になる。  
  　過去にパーセプトロンで使われた実績があるが、現在はほとんど使われることはない。  
  　<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;1\,\,\,\,(x\geq&space;0)\\0\,\,\,\,(x<&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} 1\,\,\,\,(x\geq 0)\\0\,\,\,\,(x< 0) \end{matrix}\right." />  
   
  ・シグモイド関数  
  　0〜1の間を緩やかに変化する関数。  
  　信号の強弱を伝えられ、このことがNN普及のきっかけとなった。  
  　ただし、勾配消失問題を引き起こす課題がある。  
  　<img src="https://latex.codecogs.com/gif.latex?f(u)=\frac{1}{1&plus;exp(-u)}" title="f(u)=\frac{1}{1+exp(-u)}" />  
   
  ・ReLU関数  
  　今最も使われている活性化関数  
  　シグモイド関数が抱えていた勾配消失問題の回避と、スパース化に貢献することで良い成果をもたらしている。  
  　<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;x\,\,\,\,(x>&space;0)\\&space;0\,\,\,\,(x\leq&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} x\,\,\,\,(x> 0)\\ 0\,\,\,\,(x\leq 0) \end{matrix}\right." />  
   
#### Day1-3.出力層

* 出力層とは  
  ・モデルが最終的な値を出力する層のこと。  
  ・中間層が次の層への入力を出力するのに対し、出力層は最終的に欲しい値を出力する。  

* 学習の流れ  
  (1) 訓練データを用意  
  (2) 適当な重みでNNにデータを通す。  
  (3) 出力値を正解値と比べ、誤差を求める。  
  (4) 誤差が0になるようにパラメータ(重み)を修正する。  
  
* 誤差関数  
  ・二乗和誤差  
  　<img src="https://latex.codecogs.com/gif.latex?E_{n}(w)=\frac{1}{2}\sum_{i=1}^{I}(y_{n}-d_{n})^{2}=\frac{1}{2}\left&space;\|&space;(y-d)&space;\right&space;\|^{2}" title="E_{n}(w)=\frac{1}{2}\sum_{i=1}^{I}(y_{n}-d_{n})^{2}=\frac{1}{2}\left \| (y-d) \right \|^{2}" />  
  ・交差エントロピー誤差  
  　<img src="https://latex.codecogs.com/gif.latex?E_{n}(w)=-\sum_{i=1}^{I}d_{i}log\,y_{i}" title="E_{n}(w)=-\sum_{i=1}^{I}d_{i}log\,y_{i}" />  
   
* 出力層の活性化関数  
  ・中間層との違い  
  　値の強弱  
  　中間：信号の強弱を調整する。  
  　出力：信号の大きさ(比率)はそのままに変換する。  
  ・確率出力  
  　分類が目的なら全総和が1になるようにする。  

* 出力層で使用される活性化関数  
  ・恒等関数  
  　回帰で使用する。  
  　入力値をそのまま出力する。  
  　セットで使用する誤差関数：二乗和誤差  
  　<img src="https://latex.codecogs.com/gif.latex?f(u)=u" title="f(u)=u" />  
  
  ・シグモイド関数  
  　二値分類で使用する。  
  　セットで使用する誤差関数：交差エントロピー誤差  
  　<img src="https://latex.codecogs.com/gif.latex?f(u)=\frac{1}{1&plus;exp(-u)}" title="f(u)=\frac{1}{1+exp(-u)}" />  
   
  ・ソフトマックス関数  
  　多クラス分類で使用する。  
  　セットで使用する誤差関数：交差エントロピー誤差  
  　<img src="https://latex.codecogs.com/gif.latex?f(i,u)=\frac{e^{u_{i}}}{\sum_{k=1}^{K}e^{u_{k}}}" title="f(i,u)=\frac{e^{u_{i}}}{\sum_{k=1}^{K}e^{u_{k}}}" />  
#### Day1-4.勾配降下法

* 深層学習の目的  
  ・学習を通して誤差を最小にするネットワークを構築する。  
  　手法：勾配降下法でパラメータを調整する。  
  
  ・数式モデル  
  　<img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{(t&plus;1)}=\mathbf{w}^{(t)}-\epsilon&space;\triangledown&space;E" title="\mathbf{w}^{(t+1)}=\mathbf{w}^{(t)}-\epsilon \triangledown E" />  
   
  　<img src="https://latex.codecogs.com/gif.latex?\epsilon" title="\epsilon" />：学習率  
   
  ・学習率について  
  　大きすぎる場合：発散してしまい、いつまで経っても学習が終わらなくなる。  
  　小さすぎる場合：発散のリスクは減るが、学習に時間がかかってしまうor局所解で学習が終わるリスクが増す。  
   
* 確率的勾配降下法(SGD)  
  ・学習サンプルを全て使わず、ランダムに抽出したサンプルで学習を行う手法。  
  ・メリット  
  　データが冗長な場合の計算コストを抑えられる。  
  　局所極小解に収束するリスクを抑えられる。  
  　オンライン学習ができる。(１度に全データを投入せず、逐次的に投入してパラメータを更新する手法)  
   
* ミニバッチ勾配降下法  
  ・バッチ学習  
  　全データを一気に学習する手法。  
  　全データをメモリに対して一度に載せてしまう必要がある。  
  ・数式モデル  
  　<img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{(t&plus;1)}=\mathbf{w}^{(t)}-\epsilon&space;\triangledown&space;E_{t}" title="\mathbf{w}^{(t+1)}=\mathbf{w}^{(t)}-\epsilon \triangledown E_{t}" />  
  　<img src="https://latex.codecogs.com/gif.latex?E_{t}=\frac{1}{N_{t}}\sum_{n\in&space;D_{t}}^{}E_{n}" title="E_{t}=\frac{1}{N_{t}}\sum_{n\in D_{t}}^{}E_{n}" />  
  　<img src="https://latex.codecogs.com/gif.latex?N_{t}=\left&space;|&space;D_{t}&space;\right&space;|" title="N_{t}=\left | D_{t} \right |" />  
   
  ・特徴  
  　ランダムに分割したデータの集合<img src="https://latex.codecogs.com/gif.latex?D_{t}" title="D_{t}" />に属するサンプルの平均誤差で学習を行う。  
  ・メリット  
  　SGDのメリットを損なわず、計算機の計算資源を有効利用できる。  
   
#### Day1-5.誤差逆伝播法

* 誤差逆伝播法とは  
  ・誤差を出力層側から順に微分し、前の層へ伝播させていく手法。  
  ・最小限の計算量で各パラメータでの微分値を解析的に計算する。  
  ・微分値の計算には微分の連鎖律を利用する。  
  ・計算結果(=誤差)から微分値を逆算することで、不要な再起的計算を避けて微分値を算出できる。
  
* 入力層の設計  
  ・入力値として取りうるデータ  
  　連続する実数値  
  　確率  
  　フラグ値(one-hotラベル)  
  ・入力層として受け取るべきでないデータ  
  　欠損値が多いデータ  
  　誤差の大きいデータ  
  　出力そのもの、出力を加工した情報  
  　連続性のないデータ(背番号など)  
  　無意味な数が割り当てられているデータ  

* 欠損値の扱い  
  ・入力として採用しない。(その列のみ除外)  
  ・欠損値を含む入力データを除外（その他の系列データも含めて１行丸ごと削除）  

* 数値の正規化・正則化  
  正規化：全データを0〜1の範囲に収めること。  
  正則化：正則化項を導入し、過学習を防ぐ手法。
  
* 過学習  
  ・訓練データに適応しすぎた状態のこと。  
  ・巨大なｘNNで発生しやすい。  
  ・訓練誤差とテスト誤差を見ることで発生の有無を判別可能。  
  ・ドロップアウト法など、回避するための様々な手法が提案されている。  
  
* データ集合の拡張  
  ・学習データが集まらない時に、人工的にデータを水増しする手法。  
  ・分類タスク(特に画像認識)に効果が大きい。(密度推定などではNG)  
  ・中間層にノイズを注入することでも可能。  
  ・拡張を行った学習の方が汎化性能が上がることが多い。  
  ・データ拡張まで含めてモデルの一部とすることが一般的。  
  
* 転移学習  
  ・既に学習済みのモデルを利用して、タスク固有処理のみモデルを作成する手法。  
  ・開発コストの大きい特徴量の抽出をスキップすることで開発のパフォーマンスの向上が見込める。  

#### Day2-1.勾配消失問題

* 


s
s
s
