# 深層学習 Day3, 4
## 要点のまとめ

#### Day3-1.再帰型ニューラルネットワーク(RNN)の概念

* RNNとは  
  ・時系列データに対応可能なNNのこと。  
  ・時系列データ：音声データ、テキストデータ、株価チャートなど  
  　時間順序に剃って観測され、それらに統計的相互関係が認められるデータのこと。
  
  ・ポイントは中間層にある。  
  　前回に中間層で出力された値を、次の入力に加える機構(再帰的)をもつ。  

* RNNの数学モデル  
  <img src="https://latex.codecogs.com/gif.latex?u^{t}=W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b" title="u^{t}=W_{(in)}x^{t}+Wz^{t-1}+b" />  
  <img src="https://latex.codecogs.com/gif.latex?z^{t}=f\left&space;(&space;W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b&space;\right&space;)" title="z^{t}=f\left ( W_{(in)}x^{t}+Wz^{t-1}+b \right )" />  
  　<img src="https://latex.codecogs.com/gif.latex?v^{t}=W_{(out)}z^{t}&plus;c" title="v^{t}=W_{(out)}z^{t}+c" />  
  　<img src="https://latex.codecogs.com/gif.latex?y^{t}=g\left&space;(&space;W_{(out)}z^{t}&plus;c&space;\right&space;)" title="y^{t}=g\left ( W_{(out)}z^{t}+c \right )" />  

* RNNの特徴  
  ・初期の状態と過去の時刻t-1の状態を保持し、そこから次の時刻tでの状態を再帰的に求める構造をもつ。  

* BPTT (Back Propegation Through Time)  
  ・RNNにおけるパラメータ調整法の一種。  
  ・誤差逆伝播法の一種。  

* BPTTの数学モデル  
  ・入力層→中間層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(in)}}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W_{(in)}}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;x^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(in)}}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W_{(in)}} \right ]^{T}=\delta ^{t}\left [ x^{t} \right ]^{T}" />  
  ・中間層→出力層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(out)}}=\frac{\partial&space;E}{\partial&space;v^{t}}\left&space;[&space;\frac{\partial&space;v^{t}}{\partial&space;W_{(out)}}&space;\right&space;]^{T}=\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(out)}}=\frac{\partial E}{\partial v^{t}}\left [ \frac{\partial v^{t}}{\partial W_{(out)}} \right ]^{T}=\delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  ・中間層(t-1)→中間層(t)  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;z^{t-1}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W} \right ]^{T}=\delta ^{t}\left [ z^{t-1} \right ]^{T}" />  
  ・バイアス項1  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;b}=\frac{\partial&space;E}{\partial&space;u^{t}}\frac{\partial&space;u^{t}}{\partial&space;b}=\delta&space;^{t}" title="\frac{\partial E}{\partial b}=\frac{\partial E}{\partial u^{t}}\frac{\partial u^{t}}{\partial b}=\delta ^{t}" />  
  ・バイアス項2  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;c}=\frac{\partial&space;E}{\partial&space;v^{t}}\frac{\partial&space;v^{t}}{\partial&space;c}=\delta&space;^{out,t}" title="\frac{\partial E}{\partial c}=\frac{\partial E}{\partial v^{t}}\frac{\partial v^{t}}{\partial c}=\delta ^{out,t}" />  
   
* BPTTによるパラメータ更新式  
  <img src="https://latex.codecogs.com/gif.latex?W_{(in)}^{t&plus;1}=W_{(in)}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(in)}}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;x^{t-z}&space;\right&space;]^{T}" title="W_{(in)}^{t+1}=W_{(in)}^{t}-\epsilon \frac{\partial E}{\partial W_{(in)}}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ x^{t-z} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W_{(out)}^{t&plus;1}=W_{out}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(out)}}=W_{(out)}^{t}-\epsilon&space;\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="W_{(out)}^{t+1}=W_{out}^{t}-\epsilon \frac{\partial E}{\partial W_{(out)}}=W_{(out)}^{t}-\epsilon \delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W^{t&plus;1}=W^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;z^{t-z-1}&space;\right&space;]^{T}" title="W^{t+1}=W^{t}-\epsilon \frac{\partial E}{\partial W}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ z^{t-z-1} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?b^{t-1}=b^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;b}=b^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}" title="b^{t-1}=b^{t}-\epsilon \frac{\partial E}{\partial b}=b^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}" />  
  <img src="https://latex.codecogs.com/gif.latex?c^{t-1}=c^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;c}=c^{t}-\epsilon&space;\delta&space;^{out,t}" title="c^{t-1}=c^{t}-\epsilon \frac{\partial E}{\partial c}=c^{t}-\epsilon \delta ^{out,t}" />  


#### Day3-2.LSTM

* RNNの課題  
  ・時系列を遡るほど勾配が消失していく。  
  　そのため、長い時系列データの学習が困難。  
  
* LSTMの全体像  
  ![](/image/DNN_Day3/1.png.png)
  
* CEC  
  ・勾配消失、勾配爆発の解決策として勾配が1出あればよいという発想。  
  　<img src="https://latex.codecogs.com/gif.latex?\delta&space;^{t-z-1}=\delta&space;^{t-z}\left&space;\{&space;Wf^{\,'}(u^{t-z-1})&space;\right&space;\}=1" title="\delta ^{t-z-1}=\delta ^{t-z}\left \{ Wf^{\,'}(u^{t-z-1}) \right \}=1" />  
   <img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;c^{t-1}}=\frac{\partial&space;E}{\partial&space;c^{t}}\frac{\partial&space;c^{t}}{\partial&space;c^{t-1}}=\frac{\partial&space;E}{\partial&space;c^{t}}\&space;\frac{\partial&space;}{\partial&space;c^{t-1}}\left&space;\{&space;a^{t}-c^{t-1}&space;\right&space;\}=\frac{\partial&space;E}{\partial&space;c^{t}}" title="\frac{\partial E}{\partial c^{t-1}}=\frac{\partial E}{\partial c^{t}}\frac{\partial c^{t}}{\partial c^{t-1}}=\frac{\partial E}{\partial c^{t}}\ \frac{\partial }{\partial c^{t-1}}\left \{ a^{t}-c^{t-1} \right \}=\frac{\partial E}{\partial c^{t}}" />  
   
  ・課題
  　時間依存度に関わらず重みが一律になってしまう。(NNの学習特性がなくなる)  
  　入力ゲート、出力ゲートを導入することで解決を図る。  
   
* 入力ゲート・出力ゲート  
  ・入力・出力ゲートを追加することで、それぞれのゲートへの入力値の重みを重み行列W, Uで可変可能とする。  
  ・CECで重みが一律になる課題を解決。  

* 忘却ゲート  
  ・CECの問題点として、過去の情報を全て保持する特性からいつまで経っても古い情報の影響を切り離せない問題点があった。  
  ・過去のデータが不要になったタイミングで情報を忘却する、忘却ゲートを導入することで解決を図った。  

* 覗き穴結合  
  ・CECに保存されている過去の情報を任意のタイミングで他のノードに伝播させたり、忘却させたりしたいニーズがあった。  
  　(そもそもCECのもつ値がゲート制御に活かせる構造をしていなかった。)  
  ・CEC自身の値を重みを介して伝播可能にした構造をもつ、覗き穴結合を用いて解決を図った。  

#### Day3-3.GRU

* LSTMの課題  
  ・パラメータが多く、それによって計算コストが大きくなってしまう。  

* GRUとは  
  ・Gated Recurrent Unit のこと。
  ・LSTMのパラメータを大幅に削減し、尚且つ同等以上の精度が見込める構造をしている。  
  ・メリット  
  　LSTMに比べ計算負荷が小さい。  
   
* GRUの全体像
  ![](/image/DNN_Day3/1.png.png)



aaaa
a
a
a


#### Day3-4.双方向RNN


#### Day3-5.Seq2Seq


#### Day3-6.Word2vec


#### Day3-7.Attention Mechanism



<br>

## 確認テスト

#### Day3

* PDF スライド5  




<br>

## 実装演習結果

[DNN_Day3 実装演習結果](/practice/実装演習_DNN_Day3.md)  
