# 深層学習 Day3, 4
## 要点のまとめ

#### Day3-1.再帰型ニューラルネットワーク(RNN)の概念

* RNNとは  
  ・時系列データに対応可能なNNのこと。  
  ・時系列データ：音声データ、テキストデータ、株価チャートなど  
  　時間順序に剃って観測され、それらに統計的相互関係が認められるデータのこと。
  
  ・ポイントは中間層にある。  
  　前回に中間層で出力された値を、次の入力に加える機構(再帰的)をもつ。  

* RNNの数学モデル  
  <img src="https://latex.codecogs.com/gif.latex?u^{t}=W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b" title="u^{t}=W_{(in)}x^{t}+Wz^{t-1}+b" />  
  <img src="https://latex.codecogs.com/gif.latex?z^{t}=f\left&space;(&space;W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b&space;\right&space;)" title="z^{t}=f\left ( W_{(in)}x^{t}+Wz^{t-1}+b \right )" />  
  　<img src="https://latex.codecogs.com/gif.latex?v^{t}=W_{(out)}z^{t}&plus;c" title="v^{t}=W_{(out)}z^{t}+c" />  
  　<img src="https://latex.codecogs.com/gif.latex?y^{t}=g\left&space;(&space;W_{(out)}z^{t}&plus;c&space;\right&space;)" title="y^{t}=g\left ( W_{(out)}z^{t}+c \right )" />  

* RNNの特徴  
  ・初期の状態と過去の時刻t-1の状態を保持し、そこから次の時刻tでの状態を再帰的に求める構造をもつ。  

* BPTT (Back Propegation Through Time)  
  ・RNNにおけるパラメータ調整法の一種。  
  ・誤差逆伝播法の一種。  

* BPTTの数学モデル  
  ・入力層→中間層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(in)}}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W_{(in)}}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;x^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(in)}}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W_{(in)}} \right ]^{T}=\delta ^{t}\left [ x^{t} \right ]^{T}" />  
  ・中間層→出力層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(out)}}=\frac{\partial&space;E}{\partial&space;v^{t}}\left&space;[&space;\frac{\partial&space;v^{t}}{\partial&space;W_{(out)}}&space;\right&space;]^{T}=\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(out)}}=\frac{\partial E}{\partial v^{t}}\left [ \frac{\partial v^{t}}{\partial W_{(out)}} \right ]^{T}=\delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  ・中間層(t-1)→中間層(t)  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;z^{t-1}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W} \right ]^{T}=\delta ^{t}\left [ z^{t-1} \right ]^{T}" />  
  ・バイアス項1  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;b}=\frac{\partial&space;E}{\partial&space;u^{t}}\frac{\partial&space;u^{t}}{\partial&space;b}=\delta&space;^{t}" title="\frac{\partial E}{\partial b}=\frac{\partial E}{\partial u^{t}}\frac{\partial u^{t}}{\partial b}=\delta ^{t}" />  
  ・バイアス項2  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;c}=\frac{\partial&space;E}{\partial&space;v^{t}}\frac{\partial&space;v^{t}}{\partial&space;c}=\delta&space;^{out,t}" title="\frac{\partial E}{\partial c}=\frac{\partial E}{\partial v^{t}}\frac{\partial v^{t}}{\partial c}=\delta ^{out,t}" />  
   
* BPTTによるパラメータ更新式  
  <img src="https://latex.codecogs.com/gif.latex?W_{(in)}^{t&plus;1}=W_{(in)}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(in)}}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;x^{t-z}&space;\right&space;]^{T}" title="W_{(in)}^{t+1}=W_{(in)}^{t}-\epsilon \frac{\partial E}{\partial W_{(in)}}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ x^{t-z} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W_{(out)}^{t&plus;1}=W_{out}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(out)}}=W_{(out)}^{t}-\epsilon&space;\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="W_{(out)}^{t+1}=W_{out}^{t}-\epsilon \frac{\partial E}{\partial W_{(out)}}=W_{(out)}^{t}-\epsilon \delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W^{t&plus;1}=W^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;z^{t-z-1}&space;\right&space;]^{T}" title="W^{t+1}=W^{t}-\epsilon \frac{\partial E}{\partial W}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ z^{t-z-1} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?b^{t-1}=b^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;b}=b^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}" title="b^{t-1}=b^{t}-\epsilon \frac{\partial E}{\partial b}=b^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}" />  
  <img src="https://latex.codecogs.com/gif.latex?c^{t-1}=c^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;c}=c^{t}-\epsilon&space;\delta&space;^{out,t}" title="c^{t-1}=c^{t}-\epsilon \frac{\partial E}{\partial c}=c^{t}-\epsilon \delta ^{out,t}" />  


#### Day3-2.LSTM

* RNNの課題  
  ・時系列を遡るほど勾配が消失していく。  
  　そのため、長い時系列データの学習が困難。  
  
* LSTMの全体像  
  ![](/image/DNN_Day3/1.png.png)
  
* CEC  
  ・勾配消失、勾配爆発の解決策として勾配が1出あればよいという発想。  
  　<img src="https://latex.codecogs.com/gif.latex?\delta&space;^{t-z-1}=\delta&space;^{t-z}\left&space;\{&space;Wf^{\,'}(u^{t-z-1})&space;\right&space;\}=1" title="\delta ^{t-z-1}=\delta ^{t-z}\left \{ Wf^{\,'}(u^{t-z-1}) \right \}=1" />  
   <img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;c^{t-1}}=\frac{\partial&space;E}{\partial&space;c^{t}}\frac{\partial&space;c^{t}}{\partial&space;c^{t-1}}=\frac{\partial&space;E}{\partial&space;c^{t}}\&space;\frac{\partial&space;}{\partial&space;c^{t-1}}\left&space;\{&space;a^{t}-c^{t-1}&space;\right&space;\}=\frac{\partial&space;E}{\partial&space;c^{t}}" title="\frac{\partial E}{\partial c^{t-1}}=\frac{\partial E}{\partial c^{t}}\frac{\partial c^{t}}{\partial c^{t-1}}=\frac{\partial E}{\partial c^{t}}\ \frac{\partial }{\partial c^{t-1}}\left \{ a^{t}-c^{t-1} \right \}=\frac{\partial E}{\partial c^{t}}" />  
   
  ・課題
  　時間依存度に関わらず重みが一律になってしまう。(NNの学習特性がなくなる)  
  　入力ゲート、出力ゲートを導入することで解決を図る。  
   
* 入力ゲート・出力ゲート  
  ・入力・出力ゲートを追加することで、それぞれのゲートへの入力値の重みを重み行列W, Uで可変可能とする。  
  ・CECで重みが一律になる課題を解決。  

* 忘却ゲート  
  ・CECの問題点として、過去の情報を全て保持する特性からいつまで経っても古い情報の影響を切り離せない問題点があった。  
  ・過去のデータが不要になったタイミングで情報を忘却する、忘却ゲートを導入することで解決を図った。  

* 覗き穴結合  
  ・CECに保存されている過去の情報を任意のタイミングで他のノードに伝播させたり、忘却させたりしたいニーズがあった。  
  　(そもそもCECのもつ値がゲート制御に活かせる構造をしていなかった。)  
  ・CEC自身の値を重みを介して伝播可能にした構造をもつ、覗き穴結合を用いて解決を図った。  

#### Day3-3.GRU

* LSTMの課題  
  ・パラメータが多く、それによって計算コストが大きくなってしまう。  

* GRUとは  
  ・Gated Recurrent Unit のこと。
  ・LSTMのパラメータを大幅に削減し、尚且つ同等以上の精度が見込める構造をしている。  
  ・メリット  
  　LSTMに比べ計算負荷が小さい。  
   
* GRUの全体像
  ![](/image/DNN_Day3/2.png)

#### Day3-4.双方向RNN

* 双方向RNNとは  
  ・過去の情報だけでなく、未来の情報を加味することで精度をあげたRNNの派生モデル。  
  ・入力層、中間層の順方向伝播は通常のRNNと変わらない。  
  ・出力層では時系列的に逆方向(未来から過去)の部分と結合される。  
  ・文章の推敲や翻訳に使用される。  

#### Day3-5.Seq2Seq

* Seq2Seqとは  
  ・Encoder-Decoderモデルの一種。  
  ・機械対話や機械翻訳に使われている。  
  
* Encoder RNN  
  ・ユーザーが入力したテストデータを単語などのトークンに区切って渡す構造。  
  ・Toking：文章をトークンに分割し、トークンごとのIDに分割する。  
  ・Embedding：IDから、それを表す分散表現ベクトルに変換する。  
  ・Encoder RNN：ベクトルを順にRNNに入力していく。  
  ・処理手順  
  　(1) vec1をRNNに入力 → hidden stateを出力 → vec2をRNNに入力 → hidden stateを出力 → (繰り返し)  
  　(2) 最後のvecを入力したときのhidden stateをfinal stateとしてとっておく。  
   　　　これがthought vectorと呼ばれ、入力文を意味するベクトルとなる。  
      
* Decoder RNN  
  

a
a


#### Day3-6.Word2vec


#### Day3-7.Attention Mechanism



<br>

## 確認テスト

#### Day3

* PDF スライド11  
  ・自身の考察  
  　<img src="https://latex.codecogs.com/gif.latex?H=\frac{5-3&plus;1*2}{2}&plus;1=3" title="H=\frac{5-3+1*2}{2}+1=3" />  
  　<img src="https://latex.codecogs.com/gif.latex?W=\frac{5-3&plus;1*2}{2}&plus;1=3" title="W=\frac{5-3+1*2}{2}+1=3" />  

* PDF スライド23  
  ・自身の考察  
  　現在の中間層に前回の中間層の出力を加える際にかけられる重み。  

* PDF スライド36  
  ・自身の考察  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\mathrm{d}&space;z}{\mathrm{d}&space;x}=\frac{\mathrm{d}&space;z}{\mathrm{d}&space;t}\cdot&space;\frac{\mathrm{d}&space;t}{\mathrm{d}&space;x}=2t\cdot&space;1=2(x&plus;y)" title="\frac{\mathrm{d} z}{\mathrm{d} x}=\frac{\mathrm{d} z}{\mathrm{d} t}\cdot \frac{\mathrm{d} t}{\mathrm{d} x}=2t\cdot 1=2(x+y)" />  

* PDF スライド45  
  ・自身の考察  


* PDF スライド62  
  ・自身の考察  


* PDF スライド78  
  ・自身の考察  


* PDF スライド88  
  ・自身の考察  


* PDF スライド92  
  ・自身の考察  


* PDF スライド109  
  ・自身の考察  


* PDF スライド119  
  ・自身の考察  


* PDF スライド128  
  ・自身の考察  


* PDF スライド137  
  ・自身の考察  


<br>

## 実装演習結果

[DNN_Day3 実装演習結果](/practice/実装演習_DNN_Day3.md)  
