# 深層学習 Day3, 4
## 要点のまとめ

#### Day3-1.再帰型ニューラルネットワーク(RNN)の概念

* RNNとは  
  ・時系列データに対応可能なNNのこと。  
  ・時系列データ：音声データ、テキストデータ、株価チャートなど  
  　時間順序に剃って観測され、それらに統計的相互関係が認められるデータのこと。
  
  ・ポイントは中間層にある。  
  　前回に中間層で出力された値を、次の入力に加える機構(再帰的)をもつ。  

* RNNの数学モデル  
  <img src="https://latex.codecogs.com/gif.latex?u^{t}=W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b" title="u^{t}=W_{(in)}x^{t}+Wz^{t-1}+b" />  
  <img src="https://latex.codecogs.com/gif.latex?z^{t}=f\left&space;(&space;W_{(in)}x^{t}&plus;Wz^{t-1}&plus;b&space;\right&space;)" title="z^{t}=f\left ( W_{(in)}x^{t}+Wz^{t-1}+b \right )" />  
  　<img src="https://latex.codecogs.com/gif.latex?v^{t}=W_{(out)}z^{t}&plus;c" title="v^{t}=W_{(out)}z^{t}+c" />  
  　<img src="https://latex.codecogs.com/gif.latex?y^{t}=g\left&space;(&space;W_{(out)}z^{t}&plus;c&space;\right&space;)" title="y^{t}=g\left ( W_{(out)}z^{t}+c \right )" />  

* RNNの特徴  
  ・初期の状態と過去の時刻t-1の状態を保持し、そこから次の時刻tでの状態を再帰的に求める構造をもつ。  

* BPTT (Back Propegation Through Time)  
  ・RNNにおけるパラメータ調整法の一種。  
  ・誤差逆伝播法の一種。  

* BPTTの数学モデル  
  ・入力層→中間層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(in)}}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W_{(in)}}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;x^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(in)}}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W_{(in)}} \right ]^{T}=\delta ^{t}\left [ x^{t} \right ]^{T}" />  
  ・中間層→出力層  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W_{(out)}}=\frac{\partial&space;E}{\partial&space;v^{t}}\left&space;[&space;\frac{\partial&space;v^{t}}{\partial&space;W_{(out)}}&space;\right&space;]^{T}=\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W_{(out)}}=\frac{\partial E}{\partial v^{t}}\left [ \frac{\partial v^{t}}{\partial W_{(out)}} \right ]^{T}=\delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  ・中間層(t-1)→中間層(t)  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;W}=\frac{\partial&space;E}{\partial&space;u^{t}}\left&space;[&space;\frac{\partial&space;u^{t}}{\partial&space;W}&space;\right&space;]^{T}=\delta&space;^{t}\left&space;[&space;z^{t-1}&space;\right&space;]^{T}" title="\frac{\partial E}{\partial W}=\frac{\partial E}{\partial u^{t}}\left [ \frac{\partial u^{t}}{\partial W} \right ]^{T}=\delta ^{t}\left [ z^{t-1} \right ]^{T}" />  
  ・バイアス項1  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;b}=\frac{\partial&space;E}{\partial&space;u^{t}}\frac{\partial&space;u^{t}}{\partial&space;b}=\delta&space;^{t}" title="\frac{\partial E}{\partial b}=\frac{\partial E}{\partial u^{t}}\frac{\partial u^{t}}{\partial b}=\delta ^{t}" />  
  ・バイアス項2  
  　<img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;c}=\frac{\partial&space;E}{\partial&space;v^{t}}\frac{\partial&space;v^{t}}{\partial&space;c}=\delta&space;^{out,t}" title="\frac{\partial E}{\partial c}=\frac{\partial E}{\partial v^{t}}\frac{\partial v^{t}}{\partial c}=\delta ^{out,t}" />  
   
* BPTTによるパラメータ更新式  
  <img src="https://latex.codecogs.com/gif.latex?W_{(in)}^{t&plus;1}=W_{(in)}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(in)}}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;x^{t-z}&space;\right&space;]^{T}" title="W_{(in)}^{t+1}=W_{(in)}^{t}-\epsilon \frac{\partial E}{\partial W_{(in)}}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ x^{t-z} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W_{(out)}^{t&plus;1}=W_{out}^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W_{(out)}}=W_{(out)}^{t}-\epsilon&space;\delta&space;^{out,t}\left&space;[&space;z^{t}&space;\right&space;]^{T}" title="W_{(out)}^{t+1}=W_{out}^{t}-\epsilon \frac{\partial E}{\partial W_{(out)}}=W_{(out)}^{t}-\epsilon \delta ^{out,t}\left [ z^{t} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?W^{t&plus;1}=W^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;W}=W_{(in)}^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}\left&space;[&space;z^{t-z-1}&space;\right&space;]^{T}" title="W^{t+1}=W^{t}-\epsilon \frac{\partial E}{\partial W}=W_{(in)}^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}\left [ z^{t-z-1} \right ]^{T}" />  
  <img src="https://latex.codecogs.com/gif.latex?b^{t-1}=b^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;b}=b^{t}-\epsilon&space;\sum_{z=0}^{T_{t}}\delta&space;^{t-z}" title="b^{t-1}=b^{t}-\epsilon \frac{\partial E}{\partial b}=b^{t}-\epsilon \sum_{z=0}^{T_{t}}\delta ^{t-z}" />  
  <img src="https://latex.codecogs.com/gif.latex?c^{t-1}=c^{t}-\epsilon&space;\frac{\partial&space;E}{\partial&space;c}=c^{t}-\epsilon&space;\delta&space;^{out,t}" title="c^{t-1}=c^{t}-\epsilon \frac{\partial E}{\partial c}=c^{t}-\epsilon \delta ^{out,t}" />  
  
  　
#### Day3-2.LSTM


#### Day3-3.GRU


#### Day3-4.双方向RNN


#### Day3-5.Seq2Seq


#### Day3-6.Word2vec


#### Day3-7.Attention Mechanism



<br>

## 確認テスト

#### Day3

* PDF スライド5  




<br>

## 実装演習結果

[DNN_Day3 実装演習結果](/practice/実装演習_DNN_Day3.md)  
